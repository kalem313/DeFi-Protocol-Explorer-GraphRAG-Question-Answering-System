{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from statistics import mean\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import time\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "TARGET_CLASSES = ['Protocol', 'Organization']\n",
        "\n",
        "# Small evaluation dataset (~12 DeFi-style sentences)\n",
        "DATA_CSV = '''id,text,gold_terms\n",
        "1,\"We integrated Uniswap V3 into our pipeline and measured swaps per second; audits were provided by OpenZeppelin.\",\"[{\"\"term\"\": \"\"Uniswap\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"OpenZeppelin\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "2,\"Aave reported an increase in TVL last quarter. The security review was done by CertiK.\",\"[{\"\"term\"\": \"\"Aave\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"CertiK\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "3,\"Lido's staking contracts run on Ethereum and collaborations include partnerships with Figment.\",\"[{\"\"term\"\": \"\"Lido\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Figment\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "4,\"SushiSwap announced a grant from a16z to expand liquidity incentives.\",\"[{\"\"term\"\": \"\"SushiSwap\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"a16z\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "5,\"Curve Finance focuses on stablecoin swaps; security audits by Trail of Bits increased confidence.\",\"[{\"\"term\"\": \"\"Curve Finance\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Trail of Bits\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "6,\"Balancer Labs introduced a new AMM curve; independent auditors included PeckShield.\",\"[{\"\"term\"\": \"\"Balancer Labs\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"PeckShield\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "7,\"We indexed data from MakerDAO governance proposals and referenced audits from Quantstamp.\",\"[{\"\"term\"\": \"\"MakerDAO\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Quantstamp\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "8,\"1inch aggregates liquidity across protocols like Uniswap and SushiSwap; code review by OpenZeppelin was cited.\",\"[{\"\"term\"\": \"\"1inch\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Uniswap\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"SushiSwap\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"OpenZeppelin\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "9,\"Compound governance proposals were discussed at a conference sponsored by Consensys.\",\"[{\"\"term\"\": \"\"Compound\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Consensys\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "10,\"PancakeSwap runs on BSC and was audited by CertiK earlier this year.\",\"[{\"\"term\"\": \"\"PancakeSwap\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"CertiK\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "11,\"OpenZeppelin and Trail of Bits often publish findings about vulnerabilities in DeFi protocols.\",\"[{\"\"term\"\": \"\"OpenZeppelin\"\", \"\"class\"\": \"\"Organization\"\"}, {\"\"term\"\": \"\"Trail of Bits\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "12,\"DeFi protocols such as Aave, Compound, and MakerDAO rely on audits by Quantstamp and PeckShield.\",\"[{\"\"term\"\": \"\"Aave\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Compound\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"MakerDAO\"\", \"\"class\"\": \"\"Protocol\"\"}, {\"\"term\"\": \"\"Quantstamp\"\", \"\"class\"\": \"\"Organization\"\"}, {\"\"term\"\": \"\"PeckShield\"\", \"\"class\"\": \"\"Organization\"\"}]\"\n",
        "'''\n",
        "\n",
        "# write dataset file\n",
        "with open('assignment3_eval_dataset.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(DATA_CSV)\n",
        "\n",
        "KNOWN = set([\n",
        "    'uniswap','aave','lido','sushiswap','curve finance','curve','balancer labs','1inch','compound','makerdao','pancakeswap',\n",
        "    'openzeppelin','certik','trail of bits','peckshield','quantstamp','figment','a16z','consensys'\n",
        "])\n",
        "\n",
        "def rule_based_extractor(text: str) -> List[Dict]:\n",
        "    results = []\n",
        "    lowered = text.lower()\n",
        "    for t in KNOWN:\n",
        "        if t in lowered:\n",
        "            pattern = re.compile(re.escape(t), re.IGNORECASE)\n",
        "            for m in pattern.finditer(text):\n",
        "                term = text[m.start():m.end()]\n",
        "                if t in ['openzeppelin','certik','trail of bits','peckshield','quantstamp','consensys','figment','a16z']:\n",
        "                    cls = 'Organization'\n",
        "                else:\n",
        "                    cls = 'Protocol'\n",
        "                results.append({'term': term, 'class': cls})\n",
        "\n",
        "    # detect multiword with regex (Capitalized words followed by known suffixes)\n",
        "    for m in re.finditer(r\"\\b([A-Z][a-z0-9]+(?:\\s+(?:Labs|Finance|DAO|Swap|Group|Foundation|Inc|LLC))+)\\b\", text):\n",
        "        term = m.group(1)\n",
        "        cls = 'Protocol'\n",
        "        if any(k.lower() in term.lower() for k in ['labs','foundation','inc','llc','consensys','openzeppelin','certik','trail of bits','peckshield','quantstamp']):\n",
        "            cls = 'Organization'\n",
        "        results.append({'term': term, 'class': cls})\n",
        "\n",
        "    # deduplicate\n",
        "    seen = set()\n",
        "    dedup = []\n",
        "    for r in results:\n",
        "        key = (r['term'].strip(), r['class'])\n",
        "        if key not in seen:\n",
        "            dedup.append(r)\n",
        "            seen.add(key)\n",
        "    return dedup\n",
        "\n",
        "\n",
        "def _call_gemini_api(prompt: str, response_schema: dict, system_instruction: str = None, retries=5) -> str:\n",
        "    \"\"\"Helper function to call the Gemini API with structured JSON output.\"\"\"\n",
        "\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\", \"xxxxxx_Wlb_ZfRtWqw\")\n",
        "\n",
        "    model = \"gemini-2.5-flash-preview-09-2025\"\n",
        "    base_url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent\"\n",
        "    api_url = f\"{base_url}?key={api_key}\"\n",
        "\n",
        "    contents = [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": contents,\n",
        "        \"generationConfig\": {\n",
        "            \"responseMimeType\": \"application/json\",\n",
        "            \"responseSchema\": response_schema,\n",
        "            \"temperature\": 0.0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if system_instruction:\n",
        "        payload[\"systemInstruction\"] = {\"parts\": [{\"text\": system_instruction}]}\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            if result and result.get('candidates') and result['candidates'][0].get('content'):\n",
        "                return result['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            raise ValueError(\"Gemini returned unexpected content structure or empty response.\")\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            error_details = response.text if response is not None else str(e)\n",
        "            if i < retries - 1:\n",
        "                delay = 2 ** i\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            else:\n",
        "\n",
        "                raise RuntimeError(f\"Gemini API HTTP request failed after {retries} retries. Status: {response.status_code}. Details: {error_details}\")\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            if i < retries - 1:\n",
        "                delay = 2 ** i\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to process Gemini response after {retries} retries. Error: {e}\")\n",
        "\n",
        "    return \"[]\"\n",
        "\n",
        "def llm_based_extractor(text: str) -> List[Dict]:\n",
        "    \"\"\"Extracts entities using the Gemini API with structured JSON output.\"\"\"\n",
        "\n",
        "    schema = {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"term\": {\"type\": \"STRING\", \"description\": \"The exact substring from the text.\"},\n",
        "                \"class\": {\"type\": \"STRING\", \"enum\": TARGET_CLASSES, \"description\": \"The class of the entity, either 'Protocol' or 'Organization'.\"}\n",
        "            },\n",
        "            \"required\": [\"term\", \"class\"],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a specialized DeFi Entity Extractor. Your task is to identify and extract entities \"\n",
        "        \"from the text that fit the classes 'Protocol' (DeFi platforms/projects) or 'Organization' \"\n",
        "        \"(auditors, venture capital firms, traditional companies). \"\n",
        "        \"Return a JSON array where the 'term' field is the exact substring found in the text. \"\n",
        "        \"Do not invent terms or modify the capitalization.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"Text to analyze:\\n{text}\\n\\nExtract all entities matching the classes 'Protocol' and 'Organization'.\"\n",
        "\n",
        "    try:\n",
        "        content = _call_gemini_api(prompt, schema, system_instruction=system_prompt)\n",
        "        j = json.loads(content)\n",
        "        out = []\n",
        "        for entry in j:\n",
        "            if 'term' in entry and 'class' in entry:\n",
        "                out.append({'term': entry['term'], 'class': entry['class'], 'span': None})\n",
        "        return out\n",
        "    except Exception as e:\n",
        "\n",
        "        return []\n",
        "\n",
        "\n",
        "def load_dataset(path='assignment3_eval_dataset.csv') -> List[Dict]:\n",
        "    items = []\n",
        "\n",
        "    f = StringIO(DATA_CSV)\n",
        "\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        gold = json.loads(row['gold_terms']) if row['gold_terms'] else []\n",
        "        items.append({'id': row['id'], 'text': row['text'], 'gold': gold})\n",
        "    return items\n",
        "\n",
        "\n",
        "def evaluate_extractor(extractor_fn, dataset, verbose=False):\n",
        "    \"\"\"Evaluates an extractor function against the gold standard.\"\"\"\n",
        "    TP = FP = FN = 0\n",
        "    per_doc_results = []\n",
        "    for item in dataset:\n",
        "        text = item['text']\n",
        "        gold_terms = set((g['term'].strip(), g['class']) for g in item['gold'])\n",
        "        try:\n",
        "            extracted = extractor_fn(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Extractor error on Doc {item['id']}: {e}\")\n",
        "            extracted = []\n",
        "\n",
        "        ext_set = set((e['term'].strip(), e['class']) for e in extracted)\n",
        "        tp = len(gold_terms & ext_set)\n",
        "        fp = len(ext_set - gold_terms)\n",
        "        fn = len(gold_terms - ext_set)\n",
        "        TP += tp; FP += fp; FN += fn\n",
        "        per_doc_results.append({'id': item['id'], 'tp': tp, 'fp': fp, 'fn': fn, 'gold': gold_terms, 'extracted': ext_set})\n",
        "        if verbose:\n",
        "            print(f\"Doc {item['id']}: TP={tp} FP={fp} FN={fn}\")\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1, 'TP': TP, 'FP': FP, 'FN': FN, 'per_doc': per_doc_results}\n",
        "\n",
        "\n",
        "def llm_as_judge(text: str, extracted: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Judges extracted terms using the Gemini API with structured JSON output.\"\"\"\n",
        "\n",
        "    schema = {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"term\": {\"type\": \"STRING\"},\n",
        "                \"class\": {\"type\": \"STRING\", \"enum\": TARGET_CLASSES},\n",
        "                \"judgment\": {\"type\": \"STRING\", \"enum\": [\"Correct\", \"Incorrect\"]},\n",
        "                \"explanation\": {\"type\": \"STRING\", \"description\": \"Short justification for the judgment.\"}\n",
        "            },\n",
        "            \"required\": [\"term\", \"class\", \"judgment\", \"explanation\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    items_text = '\\n'.join([f\"{i+1}. '{e['term']}' (class: {e['class']})\" for i, e in enumerate(extracted)])\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a precise annotator for DeFi extractions. Given the original text and a list of extracted terms, \"\n",
        "        \"decide for each extraction whether it is 'Correct' (the term is indeed an instance of the provided class and appears verbatim in the text) \"\n",
        "        \"or 'Incorrect'. Respond strictly with the requested JSON array.\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"Text:\\n{text}\\n\\n\"\n",
        "        f\"Extractions:\\n{items_text}\\n\\n\"\n",
        "        \"Provide your judgment (Correct/Incorrect) for each extraction.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        content = _call_gemini_api(prompt, schema, system_instruction=system_prompt)\n",
        "        j = json.loads(content)\n",
        "        return j\n",
        "    except Exception as e:\n",
        "\n",
        "        return []\n",
        "\n",
        "def compute_agreement_with_gold(dataset, extractor_fn, judge_fn=None):\n",
        "    \"\"\"Computes agreement between the extracted labels and gold labels.\"\"\"\n",
        "    gold_labels = []\n",
        "    judge_labels = []\n",
        "\n",
        "    for item in dataset:\n",
        "        text = item['text']\n",
        "        gold_terms = set((g['term'].strip(), g['class']) for g in item['gold'])\n",
        "        try:\n",
        "            extracted = extractor_fn(text)\n",
        "        except Exception as e:\n",
        "            extracted = []\n",
        "\n",
        "        if judge_fn is not None:\n",
        "            try:\n",
        "                judge_results = judge_fn(text, extracted)\n",
        "            except Exception as e:\n",
        "                judge_results = []\n",
        "        else:\n",
        "            judge_results = [{'term': e['term'], 'class': e['class'], 'judgment': 'Correct'} for e in extracted]\n",
        "\n",
        "        for jr in judge_results:\n",
        "            term_class = (jr['term'].strip(), jr['class'])\n",
        "\n",
        "            # 1 = positive, 0 = negative\n",
        "            gold = 1 if term_class in gold_terms else 0\n",
        "            judge = 1 if jr['judgment'].lower().startswith('c') else 0\n",
        "\n",
        "            gold_labels.append(gold)\n",
        "            judge_labels.append(judge)\n",
        "\n",
        "    if len(gold_labels) == 0:\n",
        "        return {'accuracy': None, 'kappa': None, 'n': 0}\n",
        "\n",
        "    accuracy = sum(1 for g,j in zip(gold_labels, judge_labels) if g==j)/len(gold_labels)\n",
        "    try:\n",
        "        kappa = cohen_kappa_score(gold_labels, judge_labels)\n",
        "    except Exception:\n",
        "        kappa = None\n",
        "\n",
        "    return {'accuracy': accuracy, 'kappa': kappa, 'n': len(gold_labels)}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    ds = load_dataset()\n",
        "    print(f'Loaded {len(ds)} documents.\\n')\n",
        "\n",
        "    print('Evaluating rule-based extractor...')\n",
        "    rule_results = evaluate_extractor(rule_based_extractor, ds, verbose=True)\n",
        "    print('\\nRule-based results:')\n",
        "    print(json.dumps({k: rule_results[k] for k in ['precision','recall','f1','TP','FP','FN']}, indent=2))\n",
        "\n",
        "    try:\n",
        "        import requests\n",
        "        _GEMINI_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        _GEMINI_AVAILABLE = False\n",
        "\n",
        "    if _GEMINI_AVAILABLE:\n",
        "        print('\\nrequests library detected. Running LLM-based extractor (Gemini API).')\n",
        "        try:\n",
        "            llm_results = evaluate_extractor(llm_based_extractor, ds)\n",
        "            print('LLM extractor results:')\n",
        "            print(json.dumps({k: llm_results[k] for k in ['precision','recall','f1','TP','FP','FN']}, indent=2))\n",
        "\n",
        "            print('\\nRunning LLM-as-judge to evaluate agreement with gold labels (may be slow)...')\n",
        "            agreement = compute_agreement_with_gold(ds, llm_based_extractor, judge_fn=llm_as_judge)\n",
        "            print('Agreement:', agreement)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "\n",
        "            print(f'Error running LLM experiments: {e}')\n",
        "        except Exception as e:\n",
        "            print(f'An unexpected error occurred during LLM experiments: {e}')\n",
        "\n",
        "    else:\n",
        "        print('\\nrequests library not available. Skipping LLM-based extractor experiments.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s81SAHP8lq_t",
        "outputId": "b4bfcc43-b4c1-4a3b-a72c-1fa0aed2c5dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Loaded 12 documents.\n",
            "\n",
            "Evaluating rule-based extractor...\n",
            "Doc 1: TP=2 FP=0 FN=0\n",
            "Doc 2: TP=2 FP=0 FN=0\n",
            "Doc 3: TP=2 FP=0 FN=0\n",
            "Doc 4: TP=2 FP=0 FN=0\n",
            "Doc 5: TP=2 FP=1 FN=0\n",
            "Doc 6: TP=2 FP=2 FN=0\n",
            "Doc 7: TP=2 FP=0 FN=0\n",
            "Doc 8: TP=4 FP=0 FN=0\n",
            "Doc 9: TP=2 FP=0 FN=0\n",
            "Doc 10: TP=2 FP=0 FN=0\n",
            "Doc 11: TP=2 FP=0 FN=0\n",
            "Doc 12: TP=5 FP=0 FN=0\n",
            "\n",
            "Rule-based results:\n",
            "{\n",
            "  \"precision\": 0.90625,\n",
            "  \"recall\": 1.0,\n",
            "  \"f1\": 0.9508196721311475,\n",
            "  \"TP\": 29,\n",
            "  \"FP\": 3,\n",
            "  \"FN\": 0\n",
            "}\n",
            "\n",
            "requests library detected. Running LLM-based extractor (Gemini API).\n",
            "LLM extractor results:\n",
            "{\n",
            "  \"precision\": 0.88,\n",
            "  \"recall\": 0.7586206896551724,\n",
            "  \"f1\": 0.8148148148148148,\n",
            "  \"TP\": 22,\n",
            "  \"FP\": 3,\n",
            "  \"FN\": 7\n",
            "}\n",
            "\n",
            "Running LLM-as-judge to evaluate agreement with gold labels (may be slow)...\n",
            "Agreement: {'accuracy': 0.9230769230769231, 'kappa': np.float64(0.0), 'n': 26}\n"
          ]
        }
      ]
    }
  ]
}